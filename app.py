import streamlit as st
from pinecone import Pinecone, ServerlessSpec
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain_pinecone import PineconeVectorStore
import os
import time

# ConfiguraciÃ³n de la pÃ¡gina
st.set_page_config(page_title="RAG con Pinecone", layout="wide")
st.title("Sistema de Preguntas y Respuestas con RAG")

# FunciÃ³n para obtener Ã­ndices directamente de Pinecone
def get_pinecone_indexes(api_key):
    try:
        pc = Pinecone(api_key=api_key)
        current_indexes = pc.list_indexes().names()
        st.write("Ãndices actuales en Pinecone:", current_indexes)
        return list(current_indexes)
    except Exception as e:
        st.error(f"Error al obtener Ã­ndices: {str(e)}")
        return []

# FunciÃ³n para limpiar todos los estados
def clear_all_states():
    for key in list(st.session_state.keys()):
        del st.session_state[key]
    st.experimental_rerun()

# Sidebar para configuraciÃ³n de credenciales
with st.sidebar:
    st.markdown("### ConfiguraciÃ³n de Credenciales")
    
    # Campo para OpenAI API Key
    openai_api_key = st.text_input(
        "OpenAI API Key",
        type="password",
        help="Introduce tu API key de OpenAI"
    )
    
    # Campos para Pinecone
    pinecone_api_key = st.text_input(
        "Pinecone API Key",
        type="password",
        help="Introduce tu API key de Pinecone"
    )
    
    # Botones de control de cachÃ© y actualizaciÃ³n
    col1, col2 = st.columns(2)
    with col1:
        if st.button("ğŸ”„ Actualizar"):
            st.experimental_rerun()
    with col2:
        if st.button("ğŸ—‘ï¸ Limpiar CachÃ©"):
            clear_all_states()
    
    # Verificar conexiÃ³n con Pinecone y obtener Ã­ndices
    if pinecone_api_key:
        try:
            st.markdown("### Estado de ConexiÃ³n")
            pc = Pinecone(api_key=pinecone_api_key)
            available_indexes = get_pinecone_indexes(pinecone_api_key)
            if available_indexes:
                st.success("âœ… Conectado a Pinecone")
                
                # Selector de Ã­ndice
                selected_index = st.selectbox(
                    "Selecciona un Ã­ndice",
                    options=available_indexes,
                    help="Selecciona el Ã­ndice de Pinecone que quieres usar"
                )
                
                # Mostrar informaciÃ³n del Ã­ndice seleccionado
                if selected_index:
                    try:
                        index = pc.Index(selected_index)
                        stats = index.describe_index_stats()
                        st.info(f"Ãndice seleccionado: {selected_index}")
                        st.write("EstadÃ­sticas del Ã­ndice:", stats)
                    except Exception as e:
                        st.error(f"Error al obtener informaciÃ³n del Ã­ndice: {str(e)}")
            else:
                st.warning("âš ï¸ No hay Ã­ndices disponibles")
                
        except Exception as e:
            st.error(f"âŒ Error de conexiÃ³n: {str(e)}")
            selected_index = None
    else:
        selected_index = None

# Variable para almacenar el estado del sistema
if 'system_initialized' not in st.session_state:
    st.session_state.system_initialized = False
if 'qa_chain' not in st.session_state:
    st.session_state.qa_chain = None

# BotÃ³n para inicializar el sistema
if pinecone_api_key and openai_api_key and selected_index:
    initialize_button = st.sidebar.button("ğŸš€ Inicializar Sistema")
else:
    st.sidebar.warning("âš ï¸ Complete las credenciales y seleccione un Ã­ndice")
    initialize_button = False

# FunciÃ³n para inicializar el sistema RAG
def initialize_rag_system():
    try:
        if not selected_index:
            raise Exception("Por favor, selecciona un Ã­ndice vÃ¡lido.")
        
        # Inicializar embeddings y modelo
        st.write("Inicializando modelos...")
        embedding_model = OpenAIEmbeddings(
            openai_api_key=openai_api_key,
            model="text-embedding-3-small"
        )
        
        # Verificar dimensiones del embedding
        st.write("Verificando dimensiones del embedding...")
        test_embed = embedding_model.embed_query("test")
        st.write(f"Dimensiones del embedding: {len(test_embed)}")
        
        # Obtener informaciÃ³n del Ã­ndice
        pc = Pinecone(api_key=pinecone_api_key)
        index = pc.Index(selected_index)
        index_info = index.describe_index_stats()
        st.write("InformaciÃ³n del Ã­ndice:", index_info)
        
        # Verificar que las dimensiones coincidan
        if 'dimension' in index_info:
            st.write(f"Dimensiones del Ã­ndice: {index_info['dimension']}")
            if len(test_embed) != index_info['dimension']:
                raise Exception(f"Las dimensiones no coinciden: Embedding ({len(test_embed)}) â‰  Ãndice ({index_info['dimension']})")
        
        llm = ChatOpenAI(
            temperature=0,
            model_name="gpt-3.5-turbo",
            openai_api_key=openai_api_key
        )
        
        # Inicializar Pinecone y verificar el Ã­ndice
        st.write("Conectando con Pinecone...")
        pc = Pinecone(api_key=pinecone_api_key)
        index = pc.Index(selected_index)
        
        # Obtener estadÃ­sticas del Ã­ndice
        stats = index.describe_index_stats()
        st.write("ğŸ“Š EstadÃ­sticas del Ã­ndice:", stats)
        if stats['total_vector_count'] == 0:
            raise Exception("El Ã­ndice estÃ¡ vacÃ­o. No hay documentos para buscar.")
        
        # Examinar contenido del Ã­ndice
        st.write("Examinando contenido del Ã­ndice...")
        
        # Obtener estadÃ­sticas detalladas
        stats = index.describe_index_stats()
        st.write("EstadÃ­sticas detalladas del Ã­ndice:")
        st.json(stats)
        
        # Intentar obtener todos los vectores del namespace
        st.write("Intentando obtener todos los vectores del namespace...")
        
        # Crear un vector de prueba para fetch_all
        dummy_vector = [0.0] * 1536  # Vector de ceros del tamaÃ±o correcto
        
        # Obtener todos los vectores
        all_vectors = index.query(
            namespace="Interfaces Multimodales y sus apps",
            vector=dummy_vector,
            top_k=100,  # Aumentar para obtener mÃ¡s resultados
            include_metadata=True,
            include_values=True
        )
        
        if all_vectors.matches:
            st.write(f"Se encontraron {len(all_vectors.matches)} vectores")
            
            # Mostrar detalles de cada vector
            for i, match in enumerate(all_vectors.matches):
                with st.expander(f"Vector {i+1}"):
                    st.write("ID:", match.id)
                    if hasattr(match, 'metadata'):
                        st.write("Metadata:", match.metadata)
                    if hasattr(match, 'score'):
                        st.write("Score:", match.score)
                    # Mostrar primeros elementos del vector si estÃ¡n disponibles
                    if hasattr(match, 'values') and match.values:
                        st.write("Primeros 5 elementos del vector:", match.values[:5])
        else:
            st.warning("No se encontraron vectores en el namespace")
        
        # Probar una bÃºsqueda con una query del dominio
        test_query = "interfaces multimodales y aplicaciones"
        query_embedding = embedding_model.embed_query(test_query)
        
        st.write("Probando bÃºsqueda con query especÃ­fica:", test_query)
        search_response = index.query(
            namespace="Interfaces Multimodales y sus apps",
            vector=query_embedding,
            top_k=5,
            include_metadata=True,
            include_values=True
        )
        
        if search_response.matches:
            st.write("Resultados encontrados:")
            for i, match in enumerate(search_response.matches):
                with st.expander(f"Resultado {i+1}"):
                    st.write("Score:", match.score)
                    st.write("ID:", match.id)
                    if hasattr(match, 'metadata'):
                        st.write("Metadata:", match.metadata)
        
        # Crear vector store con la configuraciÃ³n verificada
        vectorstore = PineconeVectorStore(
            index=index,
            embedding=embedding_model,
            text_key="text",
            namespace="Interfaces Multimodales y sus apps"
        )
        
        # Probar una bÃºsqueda simple para verificar la recuperaciÃ³n
        st.write("Probando recuperaciÃ³n de documentos...")
        test_query = "test query"
        st.write(f"Realizando bÃºsqueda de prueba con: '{test_query}'")
        test_embedding = embedding_model.embed_query(test_query)
        st.write("âœ… Embedding de prueba generado")
        
        st.write("Probando bÃºsqueda directa en vectorstore...")
        test_docs = vectorstore.similarity_search(
            test_query,
            k=3
        )
        st.write(f"ğŸ“„ Documentos recuperados en prueba: {len(test_docs)}")
        if len(test_docs) > 0:
            st.write("Ejemplo de documento recuperado:", test_docs[0].page_content[:200])
        
        # Crear retriever y chain
        st.write("Configurando retriever y chain de QA...")
        retriever = vectorstore.as_retriever(
            search_kwargs={"k": 3}
        )
        
        qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            chain_type="stuff",
            retriever=retriever,
            return_source_documents=True,
            verbose=True
        )
        
        return qa_chain
    
    except Exception as e:
        raise Exception(f"Error al inicializar el sistema: {str(e)}")

# Manejar la inicializaciÃ³n del sistema
if initialize_button:
    try:
        with st.spinner("ğŸ”„ Inicializando el sistema..."):
            st.session_state.qa_chain = initialize_rag_system()
            st.session_state.system_initialized = True
        st.success("âœ… Sistema RAG inicializado correctamente!")
    except Exception as e:
        st.error(f"âŒ {str(e)}")
        st.session_state.system_initialized = False

# Interface principal de usuario
if st.session_state.system_initialized and st.session_state.qa_chain:
    st.markdown("### ğŸ’¬ Haz tu pregunta")
    question = st.text_input("Pregunta:", key="question_input")

    if st.button("ğŸ” Obtener Respuesta"):
        if question:
            try:
                with st.spinner("ğŸ”„ Procesando tu pregunta..."):
                    st.write("ğŸ” Buscando documentos relevantes...")
                    result = st.session_state.qa_chain.invoke({"query": question})
                    
                    # Verificar el resultado
                    st.write("ğŸ“Š Estructura del resultado:", result.keys())
                    
                    # Mostrar respuesta
                    st.markdown("### ğŸ“ Respuesta:")
                    st.write(result["result"])
                    
                    # Mostrar fuentes
                    st.markdown("### ğŸ“š Fuentes utilizadas:")
                    if "source_documents" in result and result["source_documents"]:
                        for i, doc in enumerate(result["source_documents"]):
                            with st.expander(f"ğŸ“„ Fuente {i+1}"):
                                st.write("Contenido:", doc.page_content)
                                st.write("Metadata:", doc.metadata)
                    else:
                        st.warning("âš ï¸ No se encontraron documentos fuente para esta respuesta.")
                        st.write("Esto puede indicar que:")
                        st.write("1. No hay documentos similares en la base de conocimiento")
                        st.write("2. El proceso de recuperaciÃ³n no estÃ¡ funcionando correctamente")
                        st.write("3. Los embeddings no se estÃ¡n generando correctamente")
            except Exception as e:
                st.error(f"âŒ Error al procesar la pregunta: {str(e)}")
        else:
            st.warning("âš ï¸ Por favor, ingresa una pregunta.")
else:
    st.info("ğŸ‘ˆ Por favor, configura las credenciales en el panel lateral e inicializa el sistema para comenzar.")

# InformaciÃ³n adicional en el sidebar
with st.sidebar:
    st.markdown("---")
    st.markdown("### â„¹ï¸ Sobre esta aplicaciÃ³n")
    st.write("""
    Esta aplicaciÃ³n utiliza RAG (Retrieval Augmented Generation) para responder
    preguntas basÃ¡ndose en una base de conocimiento almacenada en Pinecone.
    
    El sistema:
    1. Busca informaciÃ³n relevante en la base de conocimiento
    2. Recupera los documentos mÃ¡s similares
    3. Genera una respuesta utilizando GPT y la informaciÃ³n recuperada
    """)
